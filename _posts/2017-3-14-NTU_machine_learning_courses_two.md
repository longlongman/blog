---
layout: post
title: 台大视频教程（机器学习基石）笔记（二）
date: 2017-3-14 21:40:00
category: "courses"
---
<h2>这是笔记的第二部分</h2>
<p>从这里开始我们就要解决一个十分基础同时又十分重要的问题，机器学习是否可行，这决定了我们之前做的工作是不是无用功，决定了机器学习在根本上是不是伪命题。从这里开始我们会涉及到一些机器学习中的基本证明手段，当然也有许多数学，如果读者并不对这一部分感兴趣，我在这里告诉大家结论，那就是机器学习在满足了一定条件的情况下是可行的，事实上，如果我们手中的资料确实是与我们最后想要得到的输出是有关系的，那么这时候学习通常是可行的。</p>

<h2>PART FOUR:WHY WE CAN LEARN</h2>
<h3>机器学习之不可行</h3>
<p>在我们之前的内容里，我们都没有讨论过一个问题，那就是机器学习的可行性，事实上在我们的现实生活中，确实存在着不可学习的情况，这样的情况机器没法学习，就连我们一个大活人也不一定可以学习到想要的结果。课程中老师举了一个十分容易理解但是又很能代表这种情况的例子，先看下图。</p>
<img src="https://raw.githubusercontent.com/longlongman/blog/gh-pages/images/NTU/NTU_Possibility_of_Learning.JPG">
<p>这个图实际上是在说，如果我们图片上，上方的图形我们认为它们属于-1类，而下方的图形属于+1类，现在我们给你一个新图形，我们能知道它是哪一类的么，这个问题看上去够简单吧，但是它恰恰是一个不可学习的问题。例如：今天你说我给你的图案是+1类的，因为+1类的图案都是对称图形，所以我们认为新给的这个图案是+1类的，但是如果我是老师我会说不对呀，这个图案是属于-1类的，因为这个图案和其他-1类的图案左上角的格子都是黑的，你答错了，但是如果你今天说我们新给的图形是-1类，我也是可以说你错了，理由可以是你之前想到的对称的理由，当然这里可以找到用来否定你的猜测的理由远不止这两个，所以对这个问题无论如何你都是做不对的，严格来说是只有很小几率能够做对，面对这样的问题根本没法谈学习。</p>
<p>从上面的例子里我们知道了想要从资料里学习到东西这件事情并不总是可行的，但是这也只是在我们给出如此恶劣的情况下才会发生的事情，通常我们的Learning还是可行的，之所以提到无法学习这一概念，我想是因为老师希望大家在脑海里知道学习不是适用于任何情况的，在以后遇到这样的情况是能够及时判别，不在上面浪费时间。</p>
<h3>Hoeffding's Inequality</h3>
<p>现在我们讨论一个没有这么恶劣的情况，或者说更一般的情况，在这种情况下我们认为学习是有可能的。当然，这需要理论的支撑，下面就是我们的第一个支撑：Hoeffding's Inequality（霍夫丁不等式）。</p>
<p>先放上<a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">wiki上关于霍夫丁不等式</a>的名词解释。</p>
<p>这里老师用了一个简单的例子解释了这个不等式想要和我们说些什么，现在我们假设有一个罐子，罐子里有绿色和橘色的弹珠，我们现在想要知道罐子里橘色弹珠所占的比例有多少，我们应该怎么做：（一）先用最朴素的想法，我们一颗颗数，计数地去做，OK，这样做出来的确实是最准确的，但是这样我们真的做得到么？如果我们今天罐子里的弹珠有几百万颗，乃至上亿颗，这样做无疑是十分低效的；（二）于是我们可以换一种思路，我们先假设绿色和橘色的弹珠在总体上大概是均匀分布的，那么我们取样，从这个罐子里取一把弹珠，我们对这把弹珠进行统计，算出在这把弹珠上我们橘色的占比，也许我们就能大概推算出总体上橘色弹珠的比例了，看上去很理想，可是在这里我们不要高兴得太早了，我们如果想要通过取一次弹珠就得出结论，那得要求绿色和橘色弹珠在整个罐子里完全严格的均匀分布，可是这通常是不可能的，我们只能保证大体上是均匀的，在一些局部有密集的绿弹珠或橘弹珠这和我们大体均匀是不违背的，如果今天就是运气不好就刚好取到这些密集的地方，那我们得出来的结论就很有可能是与实际情况出入较大的，既然一次试验我们的错误可能很大，那我们就做许多次，取个平均值好了，这样应该就能得到比较准确的结论了吧。（突然想到一个笑话，没有极限是用洛必达解不出来的，一次不行，就两次，我知道这个笑话说法是不对的，但是真的蛮好笑的）</p>
<p>那有没有一个定理可以给我们上面的猜测一个合理的解释呢？幸运的是，确实有，这就是霍夫丁不等式。在我们这里霍夫丁不等式可以表达如下所示：</p>
<p><img src="ht没有极限是用洛必达解不出来的没有极限是用洛必达解不出来的tp://latex.codecogs.com/gif.latex?\mathbb{P}\left&space;[&space;\left&space;|&space;\nu&space;-\mu&space;\right&space;|>&space;\varepsilon&space;\right&space;]\leqslant&space;2exp\left&space;(&space;-2\varepsilon&space;^{2}&space;N\right&space;)"/></p>
<p>这里的<img src="http://latex.codecogs.com/gif.latex?\nu"/>指的是我们多次采样后所得到的橘色球的比例，这里的<img src="http://latex.codecogs.com/gif.latex?\mu"/>则是实际上橘色球的比例，<img src="http://latex.codecogs.com/gif.latex?N"/>是我们试验的次数，<img src="http://latex.codecogs.com/gif.latex?\varepsilon"/>是我们能接受的误差范围。</p>
<p>最后我想说的是，其实霍夫丁不等式告诉了我们一个十分重要的信息，当我们试验次数足够多的时候，我们的结论误差是大误差的几率是会减小的，这是我们后面证明机器学习真的可以学到东西的基础，我们一定要熟悉这部分的知识。对于这种我们的猜测大概是正确的情况我们有一个专有的词汇来描述PCA（probably approximately correct），这里知道一下即可。</p>
<p>接下来我们把之前取弹珠的过程和我们机器学习的过程对应起来，首先在我们机器学习中所有的资料（包括在我们手上的和不在我们手上的，甚至是还没有产生的）就是罐子里所有的弹珠，而我们手上的资料则像是所有被我们取样的弹珠，在这里我们首先要知道机器学习最重要的任务是能够在我们的假说（hypothesis）集合里找到一个函数<img src="http://latex.codecogs.com/gif.latex?h(x)"/>它和我们真正的目标（target）函数<img src="http://latex.codecogs.com/gif.latex?f(x)"/>是相似的，这里忘记了的话，请看台大视频教程（机器学习基石）笔记（一），如果今天我说我们的弹珠是橘色还是绿色并不是随机的，而是由这样的一个函数，输入是弹珠，输出则是我们弹珠的颜色所决定，那么我们可以先猜测一个可能的函数，当然我们这样什么都还没有的时候给出的函数通常是不会和我想要的<img src="http://latex.codecogs.com/gif.latex?f(x)"/>接近的，也就是说我们是肯定会犯很多错误的，在这个例子中，当我们猜测的函数在某个弹珠颜色的预测上</p>