---
layout: post
title: 台大视频教程（机器学习基石）笔记（二）
date: 2017-3-14 21:40:00
category: "courses"
---
<h2>这是笔记的第二部分</h2>
<p>从这里开始我们就要解决一个十分基础同时又十分重要的问题，机器学习是否可行，这决定了我们之前做的工作是不是无用功，决定了机器学习在根本上是不是伪命题。从这里开始我们会涉及到一些机器学习中的基本证明手段，当然也有许多数学，如果读者并不对这一部分感兴趣，我在这里告诉大家结论，那就是机器学习在满足了一定条件的情况下是可行的，事实上，如果我们手中的资料确实是与我们最后想要得到的输出是有关系的，那么这时候学习通常是可行的。</p>

<h2>PART FOUR:WHY WE CAN LEARN</h2>
<h3>机器学习之不可行</h3>
<p>在我们之前的内容里，我们都没有讨论过一个问题，那就是机器学习的可行性，事实上在我们的现实生活中，确实存在着不可学习的情况，这样的情况机器没法学习，就连我们一个大活人也不一定可以学习到想要的结果。课程中老师举了一个十分容易理解但是又很能代表这种情况的例子，先看下图。</p>
<img src="https://raw.githubusercontent.com/longlongman/blog/gh-pages/images/NTU/NTU_Possibility_of_Learning.JPG">
<p>这个图实际上是在说，如果我们图片上，上方的图形我们认为它们属于-1类，而下方的图形属于+1类，现在我们给你一个新图形，我们能知道它是哪一类的么，这个问题看上去够简单吧，但是它恰恰是一个不可学习的问题。例如：今天你说我给你的图案是+1类的，因为+1类的图案都是对称图形，所以我们认为新给的这个图案是+1类的，但是如果我是老师我会说不对呀，这个图案是属于-1类的，因为这个图案和其他-1类的图案左上角的格子都是黑的，你答错了，但是如果你今天说我们新给的图形是-1类，我也是可以说你错了，理由可以是你之前想到的对称的理由，当然这里可以找到用来否定你的猜测的理由远不止这两个，所以对这个问题无论如何你都是做不对的，严格来说是只有很小几率能够做对，面对这样的问题根本没法谈学习。</p>
<p>从上面的例子里我们知道了想要从资料里学习到东西这件事情并不总是可行的，但是这也只是在我们给出如此恶劣的情况下才会发生的事情，通常我们的Learning还是可行的，之所以提到无法学习这一概念，我想是因为老师希望大家在脑海里知道学习不是适用于任何情况的，在以后遇到这样的情况是能够及时判别，不在上面浪费时间。</p>
<h3>Hoeffding's Inequality</h3>
<p>现在我们讨论一个没有这么恶劣的情况，或者说更一般的情况，在这种情况下我们认为学习是有可能的。当然，这需要理论的支撑，下面就是我们的第一个支撑：Hoeffding's Inequality（霍夫丁不等式）。</p>
<p>先放上<a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">wiki上关于霍夫丁不等式</a>的名词解释。</p>
<p>这里老师用了一个简单的例子解释了这个不等式想要和我们说些什么，现在我们假设有一个罐子，罐子里有绿色和橘色的弹珠，我们现在想要知道罐子里橘色弹珠所占的比例有多少，我们应该怎么做：（一）先用最朴素的想法，我们一颗颗数，计数地去做，OK，这样做出来的确实是最准确的，但是这样我们真的做得到么？如果我们今天罐子里的弹珠有几百万颗，乃至上亿颗，这样做无疑是十分低效的；（二）于是我们可以换一种思路，我们先假设绿色和橘色的弹珠在总体上大概是均匀分布的，那么我们取样，从这个罐子里取一把弹珠，我们对这把弹珠进行统计，算出在这把弹珠上我们橘色的占比，也许我们就能大概推算出总体上橘色弹珠的比例了，看上去很理想，可是在这里我们不要高兴得太早了，我们如果想要通过取一次弹珠就得出结论，那得要求绿色和橘色弹珠在整个罐子里完全严格的均匀分布，可是这通常是不可能的，我们只能保证大体上是均匀的，在一些局部有密集的绿弹珠或橘弹珠这和我们大体均匀是不违背的，如果今天就是运气不好就刚好取到这些密集的地方，那我们得出来的结论就很有可能是与实际情况出入较大的，既然一次试验我们的错误可能很大，那我们就做许多次，取个平均值好了，这样应该就能得到比较准确的结论了吧。（突然想到一个笑话，没有极限是用洛必达解不出来的，一次不行，就两次，我知道这个笑话说法是不对的，但是真的蛮好笑的）</p>
<p>那有没有一个定理可以给我们上面的猜测一个合理的解释呢？幸运的是，确实有，这就是霍夫丁不等式。在我们这里霍夫丁不等式可以表达如下所示：</p>
<p><img src="http://latex.codecogs.com/gif.latex?\mathbb{P}\left&space;[&space;\left&space;|&space;\nu&space;-\mu&space;\right&space;|>&space;\varepsilon&space;\right&space;]\leqslant&space;2exp\left&space;(&space;-2\varepsilon&space;^{2}&space;N\right&space;)"/></p>
<p>这里的<img src="http://latex.codecogs.com/gif.latex?\nu"/>指的是我们多次采样后所得到的橘色球的比例，这里的<img src="http://latex.codecogs.com/gif.latex?\mu"/>则是实际上橘色球的比例，<img src="http://latex.codecogs.com/gif.latex?N"/>是我们试验的次数，<img src="http://latex.codecogs.com/gif.latex?\varepsilon"/>是我们能接受的误差范围。</p>
<p>在这里其实霍夫丁不等式告诉了我们一个十分重要的信息，当我们试验次数足够多的时候，我们的结论误差是大误差的几率是会减小的，这是我们后面证明机器学习真的可以学到东西的基础，我们一定要熟悉这部分的知识。对于这种我们的猜测大概是正确的情况我们有一个专有的词汇来描述PCA（probably approximately correct），这里知道一下即可。</p>
<p>接下来让我们把抽弹珠这件事情和我们的机器学习联系起来，让我们思考一下，如果说现在我们的弹珠颜色是由一个函数<img src="http://latex.codecogs.com/gif.latex?f(x)"/>所决定的，如果我们能够找到一个“弹珠决定函数”<img src="http://latex.codecogs.com/gif.latex?h(x)"/>，由它决定的弹珠颜色，在我们每一次取样的弹珠里都比较符合实际状况，也就是由<img src="http://latex.codecogs.com/gif.latex?f(x)"/>所决定的弹珠颜色，那么根据霍夫丁不等式给我们的结论，如果我们取样次数足够多，我们就可以讲<img src="http://latex.codecogs.com/gif.latex?h(x)"/>和<img src="http://latex.codecogs.com/gif.latex?f(x)"/>相差很大的可能性是很小的，换句话来说，就是我们的<img src="http://latex.codecogs.com/gif.latex?h(x)"/>和<img src="http://latex.codecogs.com/gif.latex?f(x)"/>是相似的。这时你可能已经注意到了，这里的<img src="http://latex.codecogs.com/gif.latex?f(x)"/>就是我们的目标函数（target function），而我们的<img src="http://latex.codecogs.com/gif.latex?h(x)"/>就是我们在台大视频教程（机器学习基石）笔记（一）里讲过的，要从假说集合（hypothesis set）里挑选出来的函数，按照我们之前的讲解，我们找到了一个和目标函数<img src="http://latex.codecogs.com/gif.latex?f(x)"/>很接近的<img src="http://latex.codecogs.com/gif.latex?h(x)"/>，那么我们就可以说我们学习到了如何决定一个弹珠的颜色，如果我们从这个例子出发，把一笔资料看做是弹珠，不难想象这个学习的过程可以推广到很多场景。</p>
<h2>PART FIVE:<h2>